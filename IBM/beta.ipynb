{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:47:15.280388Z",
     "start_time": "2024-08-12T07:47:13.542022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.nn as nn\n",
    "from models.nn_model import nnModel_1, nnModel_0\n",
    "from pathlib import Path\n",
    "from models.nn_model import train_deep_kernel_gp, predict_deep_kernel_gp\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--alpha', type=float, default=2.5)\n",
    "args = parser.parse_args()\n",
    "\n",
    "import json\n",
    "\n",
    "# Read the configuration from the file\n",
    "with open('experiments/config_sim_{}.json'.format(args.alpha), 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from models.utils import train_test_splitting\n",
    "\n",
    "\n",
    "class toDataLoader(Dataset):\n",
    "    def __init__(self, x_train, y_train, t_train):\n",
    "        # Generate random data for input features (x) and target variable (y)\n",
    "        self.x = x_train\n",
    "        self.t = t_train\n",
    "        self.y = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single sample as a dictionary containing input features and target variable\n",
    "        inputs = torch.hstack([self.x[idx], self.t[idx]]).float()\n",
    "        targets = self.y[idx].float()\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def trt_ctr(treatment):\n",
    "    list1, list0 = [], []\n",
    "    for index, i in enumerate(treatment):\n",
    "        if i == 1:\n",
    "            list1.append(index)\n",
    "        elif i == 0:\n",
    "            list0.append(index)\n",
    "        else:\n",
    "            raise TypeError('Invalid treatment value found')\n",
    "\n",
    "    return list1, list0\n",
    "\n",
    "\n",
    "def evaluation_nn(pred_1, pred_0, test_tau, query_step):\n",
    "    esti_tau = torch.from_numpy(pred_1 - pred_0).float()\n",
    "    pehe_test = torch.sqrt(torch.mean((esti_tau - test_tau) ** 2))\n",
    "\n",
    "    print('\\n', 'PEHE at query step: {} is {}'.format(query_step, pehe_test), '\\n')\n",
    "\n",
    "    return pehe_test\n",
    "\n",
    "\n",
    "def pool_updating(idx_remaining, idx_sub_training, querying_idx):\n",
    "    # Update the training and pool set for the next AL stage\n",
    "    idx_sub_training = np.concatenate((idx_sub_training, querying_idx), axis=0)  # Update the training pool\n",
    "    # Update the remaining pool by deleting the selected data\n",
    "    mask = np.isin(idx_remaining, querying_idx,\n",
    "                   invert=True)  # Create a mask that selects the elements to delete from array1\n",
    "    idx_remaining = idx_remaining[mask]  # Update the remaining pool by subtracting the selected samples\n",
    "\n",
    "    return idx_sub_training, idx_remaining\n",
    "\n",
    "\n",
    "def one_side_uncertainty(combine_x_train, index, num_of_samples, model):\n",
    "    model.eval()\n",
    "\n",
    "    pred = model(combine_x_train[index])\n",
    "    pred_variance = pred.variance.sqrt()\n",
    "\n",
    "    uncertainty = pred_variance\n",
    "    draw_dist = uncertainty.cpu().detach().numpy()\n",
    "    # quantile_threshold = np.quantile(draw_dist, 1 - percentage)  # taking top 5% of the values\n",
    "\n",
    "    top_k = num_of_samples\n",
    "    threshold_top_k = np.partition(draw_dist, -top_k)[\n",
    "        -top_k]  # Calculate the threshold for the top 5 values instead of top 5%\n",
    "    print('Uncertainty threshold:', threshold_top_k)\n",
    "\n",
    "    acquired_idx = []\n",
    "    for idx, i in enumerate(draw_dist):\n",
    "        # print(round(i.item(),2), round(uncertainty[idx].item(),2), round(uncertainty[idx].item()/uncertainty_mean.item(),2))\n",
    "        if draw_dist[idx] >= threshold_top_k:\n",
    "            acquired_idx.append(idx)\n",
    "\n",
    "    # print('Top 5 uncertain:', draw_dist[acquired_idx])\n",
    "    acquired_idx = index[acquired_idx]\n",
    "    random_idx = np.random.permutation(len(acquired_idx))\n",
    "    acquired_idx = acquired_idx[random_idx]\n",
    "\n",
    "    num_elements_to_select = num_of_samples  # Selecting 5 values randomly as the step size\n",
    "\n",
    "    return acquired_idx[:num_elements_to_select], threshold_top_k\n",
    "\n",
    "\n",
    "# Function to calculate pairwise Euclidean distance in batches\n",
    "def pairwise_distances_in_batches(data, batch_size=500):\n",
    "    n = data.size(0)\n",
    "    distances = torch.zeros(n, n, device=data.device)\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        for j in range(i, n, batch_size):\n",
    "            end_i = min(i + batch_size, n)\n",
    "            end_j = min(j + batch_size, n)\n",
    "            diff = data[i:end_i].unsqueeze(1) - data[j:end_j].unsqueeze(0)\n",
    "            dist_batch = torch.sqrt(torch.sum(diff ** 2, dim=-1))\n",
    "            distances[i:end_i, j:end_j] = dist_batch\n",
    "            if i != j:\n",
    "                distances[j:end_j, i:end_i] = dist_batch.T\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_size = 0.1\n",
    "num_trial = 1\n",
    "al_step = 1\n",
    "warm_up = 50\n",
    "num_of_samples = 25\n",
    "seed = args.seed"
   ],
   "id": "d59c73355ce32a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--alpha ALPHA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/hl506-8850/.local/share/jupyter/runtime/kernel-4022b6de-abaa-4a87-b7ac-3f62343a72de.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl506-8850/anaconda3/envs/MACAL/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if num_trial == 1:\n",
    "    # for seed in range(num_trial):\n",
    "\n",
    "    print('Trial:', seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    combine_x_train, \\\n",
    "        combine_x_test, \\\n",
    "        combined_y_train, \\\n",
    "        combine_x_valid, \\\n",
    "        combined_y_valid, \\\n",
    "        tau_test, T_train, \\\n",
    "        T_valid, \\\n",
    "        T_test, \\\n",
    "        y_std = train_test_splitting(seed, device=device)\n",
    "\n",
    "    idx_pool = np.random.permutation(len(combine_x_train))  # Global dataset index\n",
    "    idx_sub_training = idx_pool[:warm_up]  # Global dataset index\n",
    "    idx_remaining = idx_pool[warm_up:]  # Global dataset index\n",
    "\n",
    "    sub_training_1, sub_training_0 = trt_ctr(T_train[idx_sub_training])\n",
    "    remaining_1, remaining_0 = trt_ctr(T_train[idx_remaining])\n",
    "\n",
    "    # Initialize the data-limited starting size as 20% of whole treated training set\n",
    "    idx_sub_training_1 = idx_sub_training[sub_training_1]  # 20% as initial\n",
    "    idx_remaining_1 = idx_remaining[remaining_1]  # 20% left for querying\n",
    "\n",
    "    # Initialize the data-limited starting size as 20% of whole control training set\n",
    "    idx_sub_training_0 = idx_sub_training[sub_training_0]  # 10% as initial\n",
    "    idx_remaining_0 = idx_remaining[remaining_0]  # 90% left for querying\n",
    "\n",
    "    acquired_treated, acquired_control = None, None\n",
    "    error_list = []\n",
    "    num_of_acquire = [len(idx_sub_training_1) + len(idx_sub_training_0)]\n",
    "\n",
    "    for query_step in range(al_step):\n",
    "        train_x_1, train_y_1 = combine_x_train[idx_sub_training_1], combined_y_train[idx_sub_training_1]\n",
    "        train_x_0, train_y_0 = combine_x_train[idx_sub_training_0], combined_y_train[idx_sub_training_0]\n",
    "        print(\"Number of data used for training in treated and control:\", len(idx_sub_training_1),\n",
    "              len(idx_sub_training_0))\n",
    "\n",
    "        print(combine_x_train.shape)\n",
    "\n",
    "        # Compute pairwise distances in batches\n",
    "        batch_size = 500  # Adjust the batch size according to your memory limits\n",
    "        distances = pairwise_distances_in_batches(combine_x_train, batch_size)\n",
    "\n",
    "        # Normalize the distances\n",
    "        max_distance = torch.max(distances)\n",
    "        normalized_distances = distances / max_distance\n",
    "\n",
    "        print(normalized_distances.max())\n",
    "\n",
    "        # Step 1: Create a mask that excludes the diagonal\n",
    "        mask = torch.eye(normalized_distances.size(0), dtype=torch.bool).to(device)\n",
    "\n",
    "        # Step 2: Apply the mask to exclude diagonal elements\n",
    "        masked_distances = normalized_distances.masked_fill(mask, float('inf'))\n",
    "\n",
    "        # Step 3: Find the minimum value in the masked distance matrix\n",
    "        min_value = masked_distances.min()\n",
    "\n",
    "        print(\"Minimum non-diagonal distance:\", min_value.item())\n",
    "\n",
    "        # Step 1: Flatten the distance matrix\n",
    "        distances_flat = normalized_distances.flatten()\n",
    "\n",
    "        # Step 2: Exclude diagonal elements (zeros)\n",
    "        # Create a mask for non-diagonal elements\n",
    "        mask = ~torch.eye(normalized_distances.size(0), dtype=torch.bool).flatten().to(normalized_distances.device)\n",
    "        non_diagonal_distances = distances_flat[mask]\n",
    "\n",
    "        # Step 3: Convert to numpy for easy plotting\n",
    "        non_diagonal_distances_np = non_diagonal_distances.cpu().numpy()\n",
    "\n",
    "        # Step 4: Plot the histogram\n",
    "        plt.hist(non_diagonal_distances_np, bins=50, edgecolor='black')\n",
    "        plt.title(\"Distribution of Non-Diagonal Distances\")\n",
    "        plt.xlabel(\"Distance\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        #plt.show()\n",
    "\n",
    "        # Assume 'distances' is your distance matrix of shape [9540, 9540]\n",
    "\n",
    "        # Step 1: Create an adjacency list or matrix\n",
    "        n = normalized_distances.size(0)\n",
    "        adjacency_list = {i: [] for i in range(n)}\n",
    "\n",
    "        # Step 2: Populate the adjacency list based on the distance threshold\n",
    "        threshold = 0.1\n",
    "\n",
    "        # Step 2: Create a mask where distances are less than the threshold and not on the diagonal\n",
    "        # This mask identifies the valid edges\n",
    "        valid_edges = (normalized_distances < threshold) & (torch.eye(normalized_distances.size(0), device=device) == 0)\n",
    "\n",
    "        # Step 3: Get the indices of the valid edges\n",
    "        rows, cols = torch.where(valid_edges)\n",
    "\n",
    "        # Step 4: Construct the directed graph using NetworkX\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Add nodes (optional, as NetworkX will add them automatically with edges)\n",
    "        G.add_nodes_from(range(normalized_distances.size(0)))\n",
    "\n",
    "        # Add edges based on the indices\n",
    "        edges = list(zip(cols.cpu().numpy(), rows.cpu().numpy()))  # direction from cols to rows\n",
    "        G.add_edges_from(edges)\n",
    "\n",
    "        print('Graph construction completed')\n",
    "\n",
    "        # Compute out-degree for each vertex\n",
    "        out_degrees = dict(G.out_degree())\n",
    "        #print(out_degrees)\n",
    "\n",
    "        # Find the node with the highest out-degree\n",
    "        max_out_degree_node = max(out_degrees, key=out_degrees.get)\n",
    "        print(f\"Node with the highest out-degree: {max_out_degree_node}\")\n",
    "\n",
    "        # Step 1: Identify the direct neighbors (nodes with an incoming edge from the selected node)\n",
    "        neighbors = list(G.successors(max_out_degree_node))\n",
    "\n",
    "        # Step 2: Identify all nodes to remove incoming edges from (include the picked node)\n",
    "        nodes_to_modify = [max_out_degree_node] + neighbors\n",
    "\n",
    "        print(len(nodes_to_modify))\n",
    "\n",
    "        # Step 3: Remove all incoming edges to these nodes\n",
    "        for node in nodes_to_modify:\n",
    "            incoming_edges = list(G.in_edges(node))  # Get all incoming edges to the node\n",
    "            G.remove_edges_from(incoming_edges)  # Remove the incoming edges\n",
    "\n",
    "        # Optional: Verify by checking in-degrees after removal\n",
    "        # for node in nodes_to_modify:\n",
    "        #    print(f\"In-degree of node {node} after removal: {G.in_degree(node)}\")"
   ],
   "id": "d1247b68c6f20caf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
